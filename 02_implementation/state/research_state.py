"""
State management for the Autonomous Research Assistant.
Defines the shared state structure used across all agents.
"""

from typing import TypedDict, Optional, Annotated
from operator import add
from datetime import datetime


class ResearchTask(TypedDict):
    """Individual research subtask"""
    task_id: str
    description: str
    topic: str
    priority: int
    status: str  # 'pending', 'in_progress', 'completed'


class Finding(TypedDict):
    """Research finding from external sources"""
    finding_id: str
    task_id: str
    content: str
    source_url: str
    source_title: str
    source_quality: float  # 0-1 score
    relevance_score: float  # 0-1 score
    timestamp: str


class Insight(TypedDict):
    """Strategic insight generated by analyst"""
    insight_id: str
    category: str  # 'market_trend', 'competitor_analysis', 'opportunity', 'risk'
    description: str
    supporting_findings: list[str]  # finding_ids
    confidence: float  # 0-1 score
    priority: int  # 1-5


class Recommendation(TypedDict):
    """Actionable recommendation"""
    recommendation_id: str
    title: str
    description: str
    rationale: str
    supporting_insights: list[str]  # insight_ids
    impact: str  # 'high', 'medium', 'low'
    effort: str  # 'high', 'medium', 'low'


class ErrorEntry(TypedDict):
    """Error tracking"""
    timestamp: str
    agent: str
    error_type: str
    message: str
    recoverable: bool


class QualityMetrics(TypedDict):
    """Quality assessment metrics"""
    coverage_score: float  # How well we've covered the topic
    source_quality_score: float  # Average quality of sources
    insight_depth_score: float  # Depth of analysis
    timestamp: str


class ResearchState(TypedDict):
    """
    Complete state for the research workflow.
    Shared across all agents via LangGraph.
    """
    # Input
    query: str
    requirements: dict  # Additional parameters like depth, focus areas, etc.

    # Planning (Orchestrator)
    research_plan: list[ResearchTask]
    current_task: Optional[ResearchTask]

    # Research outputs (Research Agent)
    findings: Annotated[list[Finding], add]  # Append-only
    covered_topics: Annotated[list[str], add]  # Track what we've researched
    search_queries_used: Annotated[list[str], add]  # Avoid duplicate searches

    # Analysis outputs (Analyst Agent)
    # Note: These are replaced on each iteration, not appended
    insights: list[Insight]
    recommendations: list[Recommendation]

    # Quality metrics
    quality_metrics: Optional[QualityMetrics]

    # Coordination
    iteration_count: int
    needs_more_research: bool
    max_iterations: int  # Safety limit

    # Error handling
    errors: Annotated[list[ErrorEntry], add]
    has_critical_error: bool

    # Cost tracking
    api_calls_count: int
    estimated_cost: float

    # Final output
    final_report: Optional[dict]
    status: str  # 'planning', 'researching', 'analyzing', 'completed', 'failed'

    # Metadata
    started_at: str
    completed_at: Optional[str]


def create_initial_state(query: str, requirements: dict = None) -> ResearchState:
    """
    Create initial state for a new research request.

    Args:
        query: The research question/topic
        requirements: Optional parameters like max_iterations, focus_areas, etc.

    Returns:
        Initialized ResearchState
    """
    return ResearchState(
        # Input
        query=query,
        requirements=requirements or {},

        # Planning
        research_plan=[],
        current_task=None,

        # Research outputs
        findings=[],
        covered_topics=[],
        search_queries_used=[],

        # Analysis outputs
        insights=[],
        recommendations=[],

        # Quality metrics
        quality_metrics=None,

        # Coordination
        iteration_count=0,
        needs_more_research=True,
        max_iterations=requirements.get("max_iterations", 3) if requirements else 3,

        # Error handling
        errors=[],
        has_critical_error=False,

        # Cost tracking
        api_calls_count=0,
        estimated_cost=0.0,

        # Final output
        final_report=None,
        status="planning",

        # Metadata
        started_at=datetime.now().isoformat(),
        completed_at=None
    )


def calculate_quality_metrics(state: ResearchState) -> QualityMetrics:
    """
    Calculate quality metrics based on current state.

    Args:
        state: Current research state

    Returns:
        QualityMetrics with scores
    """
    findings = state.get("findings", [])
    insights = state.get("insights", [])
    plan = state.get("research_plan", [])

    # Coverage: How many planned tasks are completed
    if plan:
        completed_tasks = [t for t in plan if t.get("status") == "completed"]
        coverage_score = len(completed_tasks) / len(plan)
    else:
        coverage_score = 0.0

    # Source quality: Average quality of all findings
    if findings:
        source_quality_score = sum(f.get("source_quality", 0) for f in findings) / len(findings)
    else:
        source_quality_score = 0.0

    # Insight depth: Combination of insight count and confidence
    if insights:
        avg_confidence = sum(i.get("confidence", 0) for i in insights) / len(insights)
        # Need at least 3 insights with good confidence
        insight_depth_score = min(1.0, (len(insights) / 5.0) * avg_confidence)
    else:
        insight_depth_score = 0.0

    return QualityMetrics(
        coverage_score=coverage_score,
        source_quality_score=source_quality_score,
        insight_depth_score=insight_depth_score,
        timestamp=datetime.now().isoformat()
    )


def is_quality_sufficient(metrics: QualityMetrics, min_thresholds: dict = None) -> bool:
    """
    Determine if quality metrics meet minimum thresholds.

    Args:
        metrics: Current quality metrics
        min_thresholds: Optional custom thresholds

    Returns:
        True if quality is sufficient
    """
    thresholds = min_thresholds or {
        "coverage_score": 0.8,
        "source_quality_score": 0.7,
        "insight_depth_score": 0.75
    }

    return (
        metrics["coverage_score"] >= thresholds["coverage_score"] and
        metrics["source_quality_score"] >= thresholds["source_quality_score"] and
        metrics["insight_depth_score"] >= thresholds["insight_depth_score"]
    )
